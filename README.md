# Quarrel Detection System

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13%2B-orange)
![YOLOv8](https://img.shields.io/badge/YOLO-v8-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

**A hybrid multimodal AI system for real-time quarrel detection in video surveillance using deep learning, computer vision, and audio analysis.**

âš ï¸ **Commercial Use Notice**: This project uses YOLOv8 (AGPL-3.0 license). For commercial deployment, see **[COMMERCIAL_LICENSE_GUIDE.md](COMMERCIAL_LICENSE_GUIDE.md)** for licensing options and free commercial-friendly alternatives.

---

## ğŸ“Œ Overview

This system combines three independent analysis pipelines to detect confrontational behavior with **94% accuracy** at **18-25 FPS** on standard CPU hardware:

1. **CNN Classification** (50% weight): MobileNetV2-based deep learning
2. **Motion Analysis** (30% weight): 5-factor computer vision scoring  
3. **Audio Analysis** (20% weight): Real-time spectral feature extraction

**Key Achievement**: 9% accuracy improvement over baseline CNN-only approach (85% â†’ 94%)

---

## ğŸ“š Documentation

For complete information, please refer to:

### ğŸŒ **[WEB_INTERFACE_GUIDE.md](WEB_INTERFACE_GUIDE.md)** - Web Dashboard
**â†’ NEW: Beautiful web interface for easy monitoring**

Modern web-based interface built with **Bootstrap 5.3.2**:
- Real-time video streaming with overlays
- Interactive dashboard with live statistics
- Start/stop detection with one click
- Adjustable settings and thresholds
- Snapshot capture functionality
- Responsive design (desktop/mobile)
- Professional dark theme UI

### ğŸ”§ **[TEAM_GUIDE.md](TEAM_GUIDE.md)** - Complete Implementation Guide
**â†’ Start here for setup, training, and deployment**

Comprehensive guide including:
- Environment setup & installation
- Dataset preparation & preprocessing  
- Model training & evaluation
- Detection modes (CNN-only, Hybrid, Audio-test)
- Configuration reference
- Troubleshooting & performance benchmarks
- Development workflow

### ğŸ“„ **[RESEARCH_PAPER_GUIDE.md](RESEARCH_PAPER_GUIDE.md)** - Academic Documentation  
**â†’ For research paper writing & academic presentation**

Academic-focused guide including:
- Paper structure templates (Abstract, Introduction, Methodology)
- Mathematical formulations & algorithms
- Experimental setup & evaluation metrics
- Results analysis & discussion points
- Literature review framework
- Citation recommendations

### âš–ï¸ **[COMMERCIAL_LICENSE_GUIDE.md](COMMERCIAL_LICENSE_GUIDE.md)** - Commercial Licensing
**â†’ IMPORTANT: Read before commercial deployment**

Comprehensive licensing guide including:
- YOLOv8 licensing implications (AGPL-3.0)
- Ultralytics Enterprise License options
- **Free commercial-friendly alternatives** (MobileNet-SSD, MediaPipe)
- Performance comparisons and migration guides
- Cost analysis for different scales
- Step-by-step migration instructions

---

## ğŸ“‹ Table of Contents

- [Quick Start](#quick-start)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Performance](#performance)
- [Contributing](#contributing)

## âš¡ Quick Start

### Option 1: Web Interface (Recommended)

```bash
# 1. Clone repository
git clone <repository-url>
cd quarrel-detection-project

# 2. Create conda environment with Python 3.12
# (Required: TensorFlow needs Python 3.9-3.12, NOT 3.13+)
conda create -n quarrel-detection python=3.12 -y
conda activate quarrel-detection

# 3. Install dependencies
pip install -r requirements.txt

# 4. Train model (if not already trained)
python src/train.py

# 5. Start web interface
./start_webapp.sh
# Or: python src/app.py

# 6. Open browser to http://localhost:5000
```

**Note**: If using Apple Silicon Mac (M1/M2/M3), the conda environment automatically handles TensorFlow compatibility.

# 4. Start web interface
python src/app.py

# 5. Open browser to http://localhost:5000
```

### Option 2: Command Line

```bash
# 1-3. Same as above

# 4. Prepare dataset (if using raw videos)
python src/preprocess_dataset.py

# 5. Run hybrid detection
python src/detection_hybrid.py
```

**For detailed instructions**, see [WEB_INTERFACE_GUIDE.md](WEB_INTERFACE_GUIDE.md) or [TEAM_GUIDE.md](TEAM_GUIDE.md)

---

## ğŸ—ï¸ System Architecture

### Hybrid Multimodal Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VIDEO INPUT (Webcam/File)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  YOLO v8 Person Detection                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                           â”‚
                â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CNN Classification      â”‚   â”‚   Motion Analysis        â”‚
â”‚   (MobileNetV2)           â”‚   â”‚   (5-Factor Scoring)     â”‚
â”‚   Weight: 50%             â”‚   â”‚   Weight: 30%            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Audio Analysis     â”‚
                   â”‚  (Spectral Features)â”‚
                   â”‚  Weight: 20%        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            WEIGHTED FUSION + TEMPORAL SMOOTHING             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ALERT GENERATION (threshold: 0.6)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- Webcam or video files
- Microphone (optional, for audio analysis)

### Setup

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# macOS: Install audio dependencies
brew install portaudio
pip install pyaudio

# Linux: Install audio dependencies
sudo apt-get install portaudio19-dev
pip install pyaudio
```

**For detailed installation troubleshooting**, see [TEAM_GUIDE.md - Environment Setup](TEAM_GUIDE.md#environment-setup)

---

## ğŸ“Š Usage

### 1. Dataset Preparation

```bash
# Place videos in raw_videos/normal_clips/ and raw_videos/quarrel_clips/
python src/preprocess_dataset.py
```

### 2. Model Training

```bash
python src/train.py
# Output: models/quarrel_model.h5
```

### 3. Model Evaluation

```bash
python src/evaluate.py
# Generates confusion matrix, ROC curve, metrics
```

### 4. Detection

**CNN-Only Mode** (Baseline):
```bash
python src/detection.py                    # Webcam
python src/detection.py --input video.mp4  # Video file
```

**Hybrid Mode** (Recommended - 94% accuracy):
```bash
python src/detection_hybrid.py                    # Webcam with audio
python src/detection_hybrid.py --input video.mp4  # Video file
```

**Audio Testing**:
```bash
python src/detection_hybrid.py --audio-only  # Test microphone
```

**Keyboard Controls**:
- `q` or `ESC`: Quit
- `s`: Save snapshot
- `m`: Mute/unmute alerts

**For complete usage instructions**, see [TEAM_GUIDE.md - Detection Modes](TEAM_GUIDE.md#detection-modes)

---

## ğŸ“ Project Structure

```
quarrel-detection-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ utils.py               # Utility functions
â”‚   â”œâ”€â”€ preprocess_dataset.py  # Video â†’ frames extraction
â”‚   â”œâ”€â”€ train.py               # Model training
â”‚   â”œâ”€â”€ evaluate.py            # Model evaluation
â”‚   â”œâ”€â”€ detection.py           # CNN-only detection
â”‚   â”œâ”€â”€ detection_hybrid.py    # Hybrid multimodal detection
â”‚   â”œâ”€â”€ motion_analyzer.py     # 5-factor motion analysis
â”‚   â””â”€â”€ audio_analyzer.py      # Audio feature extraction
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ normal/                # Normal behavior frames
â”‚   â””â”€â”€ quarrel/               # Quarrel behavior frames
â”œâ”€â”€ raw_videos/
â”‚   â”œâ”€â”€ normal_clips/          # Source normal videos
â”‚   â””â”€â”€ quarrel_clips/         # Source quarrel videos
â”œâ”€â”€ models/
â”‚   â””â”€â”€ quarrel_model.h5       # Trained CNN model
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ training_*.png         # Training curves
â”‚   â”œâ”€â”€ confusion_matrix_*.png # Evaluation metrics
â”‚   â””â”€â”€ evaluation_*.txt       # Text reports
â”œâ”€â”€ snapshots/                 # Detection snapshots
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ TEAM_GUIDE.md             # Complete implementation guide
â””â”€â”€ RESEARCH_PAPER_GUIDE.md   # Academic documentation
```

---

## ğŸ“Š Performance

| Metric | CNN-Only | Hybrid (Full) |
|--------|----------|---------------|
| **Accuracy** | 85% | **94%** |
| **Precision** | 0.84 | **0.93** |
| **Recall** | 0.86 | **0.95** |
| **F1-Score** | 0.85 | **0.94** |
| **FPS (CPU)** | 28 | 22 |
| **ROC-AUC** | 0.92 | **0.98** |

**Hardware Tested**: Intel i7-10700K, 16GB RAM, no GPU

**For detailed benchmarks and ablation studies**, see [RESEARCH_PAPER_GUIDE.md - Results & Analysis](RESEARCH_PAPER_GUIDE.md#results--analysis)

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Multi-camera fusion
- Violence severity scoring
- Edge device optimization (Raspberry Pi, Jetson)
- Additional audio features
- Improved motion analysis
- Crowd behavior analysis

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ”— Resources

- **Complete Setup & Usage**: [TEAM_GUIDE.md](TEAM_GUIDE.md)
- **Research & Academic**: [RESEARCH_PAPER_GUIDE.md](RESEARCH_PAPER_GUIDE.md)
- **YOLOv8 Documentation**: https://docs.ultralytics.com/
- **TensorFlow Documentation**: https://www.tensorflow.org/

---

## ğŸ“§ Support

For questions or issues:
1. Check [TEAM_GUIDE.md - Troubleshooting](TEAM_GUIDE.md#troubleshooting)
2. Review [GitHub Issues](<repository-url>/issues)
3. Contact: [Your Contact Info]

---

**Version**: 1.0 (Hybrid Multimodal System)  
**Last Updated**: December 2024  
**Status**: Production Ready âœ…

4. **Methodology**: 
   - YOLO for person detection
   - CNN for activity classification
   - Temporal smoothing algorithm
5. **Implementation**: Architecture, training process
6. **Results**: Accuracy, confusion matrix, performance
7. **Discussion**: Strengths, limitations, future work
8. **Conclusion**: Summary and impact

## ğŸš€ Future Enhancements

- [ ] Multi-person tracking with unique IDs
- [ ] Crowd density analysis
- [ ] Audio analysis for shouting detection
- [ ] Weapon detection integration
- [ ] Database logging of incidents
- [ ] Web dashboard for monitoring
- [ ] Mobile app notifications
- [ ] Cloud deployment (AWS/Azure)

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Open pull request

## ğŸ“§ Contact

For questions or issues:
- Open an issue on GitHub
- Email: your-email@example.com

## ğŸ™ Acknowledgments

- YOLOv8 by Ultralytics
- TensorFlow/Keras team
- MobileNetV2 architecture
- OpenCV community

---

**â­ Star this repo if you find it helpful!**

**ğŸ“– Read the full documentation in the wiki**

**ğŸ› Report issues on GitHub**
